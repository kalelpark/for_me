{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Weight Initialization(가중치 초기화)\n<img src = 'https://cdn.analyticsvidhya.com/wp-content/uploads/2021/05/291611_dmRbfOye2PcDMl2-bQazVg.jpeg'></src>\n\n\n## - In Neural Network learning, neural network differ depending on intial weights\n    - 신경망 학습에서 가중치 초깃값을 어떻게 정하느냐에 따라서 신경망 학습이 달라집니다.\n\n# Content\n\n## - Random Initialization(랜덤 초기화)\n\n## - Xavier Initialization(사비에르 초기화)\n\n## - He Initialization(He 초기화)\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef Relu(x):\n    return np.maximum(0, x)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef Weight_Initialization(Initialization, activate = 'sigmoid', deviation = 1):\n    input_data = np.random.randn(1000, 100)\n    node_num = 100\n    hidden_layer = 5\n    activations = {}\n    \n    x = input_data\n    \n    for i in range(hidden_layer):\n        if i != 0:\n            x = activations[i-1]\n        \n        if Initialization == 'Random':\n            w = np.random.randn(node_num, node_num) * deviation\n        elif Initialization == 'Xavier':\n            w = np.random.randn(node_num, node_num) * np.sqrt(1/node_num)\n        elif Initialization == 'He':\n            w = np.random.randn(node_num, node_num) * np.sqrt(2/node_num)\n        \n        a = np.dot(x, w)\n        \n        if activate == 'sigmoid':\n            z = sigmoid(a)\n        elif activate == 'Relu':\n            z = Relu(a)\n        elif activate == 'tanh':\n            z = tanh(a)\n        \n        activations[i] = z\n            \n    \n    for i, a in activations.items():\n        plt.subplot(1, len(activations), i+1)\n        plt.title(str(i + 1) + '-layer')\n        if i != 0:\n            plt.yticks([], [])\n        if Initialization == 'He':\n            plt.ylim(0, 7000)\n        plt.xlim(0.1, 1.0)\n        plt.hist(a.flatten(), 30, range=(0, 1))\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:38:24.689467Z","iopub.execute_input":"2022-01-04T15:38:24.690138Z","iopub.status.idle":"2022-01-04T15:38:24.705187Z","shell.execute_reply.started":"2022-01-04T15:38:24.690054Z","shell.execute_reply":"2022-01-04T15:38:24.704244Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Random Initialization(랜덤 초기화)\n<img src = 'https://www.researchgate.net/profile/Aleksandra-Vuckovic/publication/3978633/figure/fig2/AS:394699887136770@1471115194381/Feed-forward-neural-network-with-sigmoid-activation-function-X-i-i-1P-input.png'></src>\n\n##### ENG\n- Best practices recommend using a random set, with initial bias of zero.\n\n- we need to break the symmetry, It makes each neuron perform a different computation.\n\n- if symmetry condition, training can be severely penalized or even impossible.\n\n* Disadvantage\n    - if deviation == 1, Due to Vanishing Gradient Problem, Neural Network doesn't learning.\n    - if deviation == 0.01, The expression of the Activation value distribution is limited.\n\n\n##### KOR\n- 최고의 학습방법은 초기 편향인 0인 random set을 활용하는 것입니다,\n- 각 뉴련이 다르게 계산을 해야 하기 때문에, 대칭성을 깨야 합니다.\n- 만약 대칭성을 가지게 되는 경우, 학습이 불가능하거나 이상할 것입니다.\n\n* 단점\n    - 표준편차가 1인 경우, 기울기 소실 문제가 발생해, 학습이 제대로 실행되지 않는다.\n    - 표준편차가 0.01인 경우, 표현력이 제한됩니다.\n\nreference link : https://www.coursera.org/lecture/machine-learning/random-initialization-drcBh","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:21:37.500841Z","iopub.execute_input":"2022-01-04T09:21:37.501389Z","iopub.status.idle":"2022-01-04T09:21:37.508017Z","shell.execute_reply.started":"2022-01-04T09:21:37.501351Z","shell.execute_reply":"2022-01-04T09:21:37.507145Z"}}},{"cell_type":"code","source":"# Random Initialization\nWeight_Initialization(Initialization='Random', activate='sigmoid')\n# Deviation = 0.01\nWeight_Initialization(Initialization='Random', activate='sigmoid', deviation=0.01)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T14:49:29.111730Z","iopub.execute_input":"2022-01-04T14:49:29.112595Z","iopub.status.idle":"2022-01-04T14:49:30.624278Z","shell.execute_reply.started":"2022-01-04T14:49:29.112536Z","shell.execute_reply":"2022-01-04T14:49:30.623597Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Xavier Initialization(사비에르 초기화)\n<img src = 'https://blog.kakaocdn.net/dn/bQn1My/btqB1cEniE0/to4hTdl9SzGF6zuv9MsIO1/img.png'></src>\n##### ENG\n- It's called Xavier Initilization or Glorot Initilization\n- Initialize weights considering node_input and node_output\n- It's often used tanh activation function\n- it's preserve the backpropagated signal as well\n\n<b>Disadvantage</b>\n   - It doesn't working well in Relu activation function.\n\n\n##### KOR\n- Xavier Initilization 또는 Glorot Initilization라고 불립니다.\n- 입력 노드의 개수 와 출력 노드의 개수를 고려하여, 가중치를 초기화합니다.\n- 활성화 함수로 Tanh를 주로 사용합니다.\n- 오차역전파를 보전합니다.\n\n<b>단점</b>\n- Relu 활성화 함수에서 잘 작동하지 않습니다.\n\nreference link : https://prateekvishnu.medium.com/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528","metadata":{}},{"cell_type":"code","source":"# Xavier Initialization\nWeight_Initialization(Initialization='Xavier', activate='sigmoid')\n# activation Relu\nWeight_Initialization(Initialization='Xavier', activate='Relu')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:38:38.314172Z","iopub.execute_input":"2022-01-04T15:38:38.314514Z","iopub.status.idle":"2022-01-04T15:38:39.797214Z","shell.execute_reply.started":"2022-01-04T15:38:38.314478Z","shell.execute_reply":"2022-01-04T15:38:39.795981Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# He Initialization(He 초기화)\n<img src = 'https://mblogthumb-phinf.pstatic.net/MjAxOTA3MzBfNjQg/MDAxNTY0NDcwNTU1MDg2.5-fh3tQMhy_9dBHH2URK2w1IUxoemhGGvi2LB5DQJ5kg.qSi4AGif9AqyjjQudKYs7DGzTKVxUuvSxF_AcStEDo0g.PNG.sohyunst/SE-b8654ce8-25af-4ff8-9c47-3f2fd5b2a884.png?type=w800'></src>\n##### ENG\n- It's used Relu activation function\n- He Initialization is one of the methods you can choose to bring the variance of those outputs to approximately one\n- Bias disappears in Relu activation function\n\n##### KOR\n- 주로 Relu activation function을 사용합니다.\n- He Intialization은 출력의 분산값을 대략 1로 만듭니다.\n- Relu activation function 에서 편향이 사라집니다.","metadata":{}},{"cell_type":"code","source":"# He Initialization\nWeight_Initialization(Initialization='He', activate='Relu')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:38:31.554543Z","iopub.execute_input":"2022-01-04T15:38:31.555184Z","iopub.status.idle":"2022-01-04T15:38:32.454022Z","shell.execute_reply.started":"2022-01-04T15:38:31.555129Z","shell.execute_reply":"2022-01-04T15:38:32.453056Z"},"trusted":true},"execution_count":22,"outputs":[]}]}